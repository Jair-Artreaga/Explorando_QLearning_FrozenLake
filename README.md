# Q-Learning en FrozenLake - AnÃ¡lisis de ParÃ¡metros de Aprendizaje por Refuerzo

## ğŸ“‹ DescripciÃ³n
Este proyecto implementa el algoritmo de Q-Learning en el entorno `FrozenLake-v1` utilizando la librerÃ­a `gymnasium`. A travÃ©s de diferentes experimentos, se estudia cÃ³mo los parÃ¡metros del algoritmo afectan el desempeÃ±o del agente. Se realizan entrenamientos con distintas configuraciones y se evalÃºa el comportamiento aprendido.

## ğŸ¯ Objetivos
Comprender el funcionamiento del algoritmo Q-Learning aplicado al entorno FrozenLake y analizar el efecto de los parÃ¡metros clave del aprendizaje por refuerzo sobre el rendimiento del agente.

## ğŸ—‚ Estructura del Proyecto
```
ğŸ“‚ Explorando_QLearning_FrozenLake
â”‚â”€â”€ ğŸ“‚ data  # Datos generados para las pruebas
â”‚â”€â”€ ğŸ“‚ results  # Resultados y anÃ¡lisis
â”‚â”€â”€ ğŸ“„ README.md  # DocumentaciÃ³n del proyecto
â”‚â”€â”€ ğŸ“„ Q_learning.ipynb  # ImplementaciÃ³n en Jupyter Notebook
```

## InstalaciÃ³n y Requisitos
Para ejecutar el proyecto, necesitas tener instalado:
- **Python 3.x**
- **Jupyter Notebook**
- **LibrerÃ­as necesarias** (se pueden instalar con pip):
  ```bash
  pip install notebook
  ```

## Uso
1. Clona este repositorio:
   ```bash
   git clone https://github.com/Jair-Artreaga/Explorando_QLearning_FrozenLake.git
   ```
2. Accede al directorio del proyecto:
   ```bash
   cd Explorando_QLearning_FrozenLake
   ```
3. Ejecuta Jupyter Notebook:
   ```bash
   jupyter notebook
   ```
4. Abre el archivo `Q_learning` y ejecuta las celdas para ver los resultados.

## Pruebas de Rendimientoo	
El experimento gandor fue el 2, el agente ganÃ³ 5 de 5 episodios (100%).
Los parÃ¡metros modificados fueron:
- Î± = 0.9 (alta tasa de aprendizaje)
- Î³ = 0.99 (factor de descuento muy alto)
- epsilon = 1.0 (exploraciÃ³n total)
- decay = 0.0002 (decadencia mÃ¡s rÃ¡pida de epsilon)
- episodes = 30000 (mayor nÃºmero de episodios de entrenamiento)

Â¿CuÃ¡l fue el comportamiento del agente al final del entrenamiento?
- El agente aprendiÃ³ muy bien debido al alto nÃºmero de episodios (30000) y el valor elevado de Î³ = 0.99, que permitiÃ³ valorar las recompensas futuras. La exploraciÃ³n fue adecuada y la polÃ­tica mejorÃ³ de manera significativa.


â€¢	Â¿CÃ³mo explicarÃ­as ese resultado en tÃ©rminos de aprendizaje por refuerzo?
- El Ã©xito del agente puede atribuirse a la combinaciÃ³n de parÃ¡metros con un alto valor de Î± que permitiÃ³ un rÃ¡pido aprendizaje de nuevas acciones, un factor de descuento alto (Î³=0.99) que promoviÃ³ la consideraciÃ³n de recompensas a largo plazo, y un nÃºmero elevado de episodios (30000) que brindÃ³ suficientes oportunidades para explorar y ajustar la polÃ­tica del agente. La decadencia mÃ¡s rÃ¡pida de epsilon tambiÃ©n permitiÃ³ una exploraciÃ³n suficiente al principio y una explotaciÃ³n efectiva al final, lo que resultÃ³ en una polÃ­tica Ã³ptima al final del entrenamiento.


## Contribuciones
Si deseas contribuir a este proyecto:
1. Haz un fork del repositorio.
2. Crea una rama con tus cambios: `git checkout -b nueva-funcionalidad`
3. Realiza un commit de los cambios: `git commit -m 'AÃ±adir nueva funcionalidad'`
4. Sube los cambios: `git push origin nueva-funcionalidad`
5. Abre un Pull Request.

## Autor
**[Roberto Jair Arteaga Valenzuela]**
